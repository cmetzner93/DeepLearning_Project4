{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1(textfile, window_size, stride):\n",
    "    # open the file as read, read text, and close file\n",
    "    file = open(textfile, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    print(text[:50])\n",
    "    \n",
    "    #  strip all of the new line characters so that we \n",
    "    # have one long sequence of characters separated only \n",
    "    # by white space\n",
    "    tokens = text.split()\n",
    "    print(tokens[:50])\n",
    "    data = ' '.join(tokens)\n",
    "    print(data[:50])\n",
    "    \n",
    "    # get sequences of characters of length window_size+1\n",
    "    sequences = []\n",
    "    for i in range(0, len(data) - (window_size + 1), stride):\n",
    "        sequence = data[i : i + window_size + 1]\n",
    "        sequences.append(sequence)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    \n",
    "    # save sequences \n",
    "    data = '\\n'.join(sequences)\n",
    "    file = open('train_data.txt', 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a day in the life\n",
      "i read the news today oh boy\n",
      "abo\n",
      "['a', 'day', 'in', 'the', 'life', 'i', 'read', 'the', 'news', 'today', 'oh', 'boy', 'about', 'a', 'lucky', 'man', 'who', 'made', 'the', 'grade', 'and', 'though', 'the', 'news', 'was', 'rather', 'sad', 'well', 'i', 'just', 'had', 'to', 'laugh', 'i', 'saw', 'the', 'photograph.', 'he', 'blew', 'his', 'mind', 'out', 'in', 'a', 'car', 'he', 'didnâ€™t', 'notice', 'that', 'the']\n",
      "a day in the life i read the news today oh boy abo\n",
      "Total Sequences: 41686\n"
     ]
    }
   ],
   "source": [
    "task1('beatles.txt', 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2(train_textfile):\n",
    "    # open the file as read, read text, and close file\n",
    "    file = open(train_textfile, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    # get list of sequences by splitting the text by new line\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # get unique characters\n",
    "    chars = sorted(list(set(text)))\n",
    "    # get mapping of character to integer values and store in a dictionary\n",
    "    char_to_i_mapping = dict((c, i) for i, c in enumerate(chars)) \n",
    "    # get vocabulary size\n",
    "    vocab_size = len(char_to_i_mapping)\n",
    "    print('Vocabulary size: %d' % vocab_size)\n",
    "    \n",
    "    # integer encode each sequence of characters using the dictionary mapping\n",
    "    sequences = []\n",
    "    for line in lines:\n",
    "        # integer encode line\n",
    "        encoded_seq = [char_to_i_mapping[char] for char in line]\n",
    "        # store\n",
    "        sequences.append(encoded_seq)\n",
    "    \n",
    "    # now separate the integer encoded sequences into input and output\n",
    "    sequences = np.array(sequences)\n",
    "    X = sequences[:,:-1]\n",
    "    y = sequences[:,-1]\n",
    "    \n",
    "    # now one-hot encode each character, meaning each character becomes a vector of length vocab_size with a 1 marked \n",
    "    # for the character and 0s elsewhere\n",
    "    sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
    "    X = np.array(sequences)\n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "    print('X shape: %s and y shape: %s' %(X.shape, y.shape))\n",
    "    \n",
    "    return(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 48\n",
      "X shape: (41686, 3, 48) and y shape: (41686, 48)\n"
     ]
    }
   ],
   "source": [
    "X, y = task2('train_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
